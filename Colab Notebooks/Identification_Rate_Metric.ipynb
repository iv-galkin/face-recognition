{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Реализация Identification Rate Metric**"
      ],
      "metadata": {
        "id": "rerTIDnbe8PK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В данном ноутбуке реализована функция, считающая Identification Rate Metric. Проверена правильность её работы на тестовых данных. Посчитаны значения метрики для двух обученных моделей:\n",
        "\n",
        "\n",
        "*   resnet34, обученной на CE loss\n",
        "*   resnet34, обученной на ArcFace loss\n",
        "\n"
      ],
      "metadata": {
        "id": "bLzw-EGCoJDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загрузка данных и импорт необходимых библиотек"
      ],
      "metadata": {
        "id": "RxFesVALi7fM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для подсчёта Identification Rate Metric используем следующие данные из датасета CelebA-500: https://disk.yandex.com/d/KN4EEkNKrF_ZXQ\n",
        "Эти данные уже разбиты на query и distractors, и в отдельном файле также находится информация о классах для картинок из query. Эти картинки заалайнены точно так же, как картинки из обучающей выборки CelebA-500"
      ],
      "metadata": {
        "id": "CxBnLr_mgyeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision.models import resnet34\n",
        "import torchvision.transforms as tt\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "k3ywiusPHxi_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6WtJmMKqzc-",
        "outputId": "79fc19f1-401e-45d8-bd33-5efda1736292"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -uq \"/content/drive/MyDrive/Datasets Deep Learning/celebA_ir.zip\" -d \"/content/celebA_ir\""
      ],
      "metadata": {
        "id": "JF8gZVUKDlHy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью следующей ячейки загрузим данные"
      ],
      "metadata": {
        "id": "wkM4TnSVhSWD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_W9BZR9ZoEn_"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import os\n",
        "\n",
        "# file with query part annotations: which image belongs to which class\n",
        "# format:\n",
        "#     image_name_1.jpg 2678\n",
        "#     image_name_2.jpg 2679\n",
        "f = open('/content/celebA_ir/celebA_ir/celebA_anno_query.csv', 'r')\n",
        "query_lines = f.readlines()[1:]\n",
        "f.close()\n",
        "query_lines = [x.strip().split(',') for x in query_lines]\n",
        "# plain list of image names from query. Neede to compute embeddings for query\n",
        "query_img_names = [x[0] for x in query_lines]\n",
        "\n",
        "# dictionary with info of which images from query belong to which class\n",
        "# format:\n",
        "#     {class: [image_1, image_2, ...]}\n",
        "query_dict = defaultdict(list)\n",
        "for img_name, img_class in query_lines:\n",
        "  query_dict[img_class].append(img_name)\n",
        "\n",
        "# list of distractor images\n",
        "distractors_img_names = os.listdir('/content/celebA_ir/celebA_ir/celebA_distractors')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(distractors_img_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rkDhyh7hE-Ls",
        "outputId": "b71ed446-50be-4388-cff4-5b9f57ffc8c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2001"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(query_img_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k1AOtYufEjv_",
        "outputId": "474f256e-c60b-4b8a-bcd7-9793980437ba"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1222"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Реализация IR metric и вспомогательных функций для её подсчёта"
      ],
      "metadata": {
        "id": "zgWBeIbLjYx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Описание того, как работает метрика IR**\n",
        "\n",
        "Создадим два набора изображений лиц: query и distractors. Никакие лица из этих наборов не должны содержаться в обучающем и валидационном датасете.\n",
        "\n",
        "1. посчитаем косинусные расстояния между лицами, соответствующими одним и тем же людям из query части. Например, пусть одному человеку соответствуют три фото в query: 01.jpg, 02.jpg, 03.jpg. Тогда считаем три косинусных расстояния между всеми тремя парами из этих фото.\n",
        "2. посчитаем косинусные расстояния между лицами, соответствующими разным людям из query части.\n",
        "3. посчитаем косинусные расстояния между всеми парами лиц из query и distractors. Т.е. пара — это (лицо из query, лицо из distractors). Всего получится |query|*|distractors| пар.\n",
        "4. Сложим количества пар, полученных на 2 и 3 шагах. Это количество false пар.\n",
        "5. Зафиксируем **FPR** (false positive rate). Пусть, например, будет 0.01. FPR, умноженный на количество false пар из шага 4 — это разрешенное количество false positives, которые мы разрешаем нашей модели. Обозначим это количество через N.\n",
        "6. Отсортируем все значения косинусных расстояний false пар. N — ое по счету значение расстояния зафиксируем как **пороговое расстояние**.\n",
        "7. Посчитаем количество positive пар с шага 1, которые имеют косинусное расстояние меньше, чем пороговое расстояние. Поделим это количество на общее количество positive пар с шага 1. Это будет TPR (true positive rate) — итоговое значение нашей метрики."
      ],
      "metadata": {
        "id": "IdYZmeNFkCmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings(model, images_list, image_folder):\n",
        "  '''\n",
        "  compute embeddings from the trained model for list of images.\n",
        "  params:\n",
        "    model: trained nn model that takes images and outputs embeddings\n",
        "    images_list: list of images paths to compute embeddings for\n",
        "  output:\n",
        "    image_folder: path to image folder of images\n",
        "    list: list of model embeddings. Each embedding corresponds to images\n",
        "          names from images_list\n",
        "  '''\n",
        "  model.eval()\n",
        "  preprocess = tt.Compose([\n",
        "        tt.Resize((224, 224)),\n",
        "        tt.ToTensor(),\n",
        "        tt.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "  embeddings_list = []\n",
        "  for image_name in tqdm.tqdm(images_list):\n",
        "    image_path = f'{image_folder}/{image_name}'\n",
        "    image = Image.open(image_path)\n",
        "\n",
        "    input_tensor = preprocess(image).unsqueeze(0)\n",
        "\n",
        "    model = model.to(device)\n",
        "    input_tensor = input_tensor.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      embedding = model(input_tensor)\n",
        "      embeddings_list.append(embedding.cpu().numpy().flatten())\n",
        "\n",
        "  return embeddings_list"
      ],
      "metadata": {
        "id": "vU00-lmzFQqq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(embedding1, embedding2):\n",
        "  dot_result = np.dot(embedding1, embedding2)\n",
        "  norm1 = np.linalg.norm(embedding1)\n",
        "  norm2 = np.linalg.norm(embedding2)\n",
        "  return dot_result / (norm1 * norm2)"
      ],
      "metadata": {
        "id": "fRj26JG8mmXp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем **пункт 1**: посчитаем косинусные расстояния между лицами, соответствующими одним и тем же людям из query части"
      ],
      "metadata": {
        "id": "IY5c3RL6kR0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_pos(query_dict, query_img_names, query_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between positive pairs from query (stage 1)\n",
        "  params:\n",
        "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
        "                the dataset. Value: images corresponding to that class\n",
        "    query_img_names: list of images names\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "  output:\n",
        "    list of floats: similarities between embeddings corresponding\n",
        "                    to the same people from query list\n",
        "  '''\n",
        "  positive_similarities = []\n",
        "\n",
        "  # словарь где ключ -- имя изображения, значение -- соответствующий эмбеддинг\n",
        "  img_to_embedding = {name: embedding for name, embedding in zip(query_img_names, query_embeddings)}\n",
        "\n",
        "  for class_name, image_names in tqdm.tqdm(query_dict.items()):\n",
        "    for i in range(len(image_names)):\n",
        "      for j in range(i + 1, len(image_names)):\n",
        "        emb1 = img_to_embedding[image_names[i]]\n",
        "        emb2 = img_to_embedding[image_names[j]]\n",
        "        similarity = compute_cosine_similarity(emb1, emb2)\n",
        "        positive_similarities.append(similarity)\n",
        "\n",
        "  return positive_similarities"
      ],
      "metadata": {
        "id": "ufNeWFcGFyuC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализуем **пункт 2**: посчитаем косинусные расстояния между лицами, соответствующими разным людям из query части."
      ],
      "metadata": {
        "id": "IeQwH2_5kX5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_neg(query_dict, query_img_names, query_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between negative pairs from query (stage 2)\n",
        "  params:\n",
        "    query_dict: dict {class: [image_name_1, image_name_2, ...]}. Key: class in\n",
        "                the dataset. Value: images corresponding to that class\n",
        "    query_img_names: list of images names\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "  output:\n",
        "    list of floats: similarities between embeddings corresponding\n",
        "                    to different people from query list\n",
        "  '''\n",
        "  negative_similarities = []\n",
        "\n",
        "  # словарь где ключ -- имя изображения, значение -- соответствующий эмбеддинг\n",
        "  img_to_embedding = {name: embedding for name, embedding in zip(query_img_names, query_embeddings)}\n",
        "\n",
        "  # будем хранить в списке названия всех классов для удобства итерации\n",
        "  img_classes_list = list(query_dict.keys())\n",
        "\n",
        "  for i in tqdm.tqdm(range(len(img_classes_list))):\n",
        "    class1_name = img_classes_list[i]\n",
        "    class1_images = query_dict[class1_name]\n",
        "    for j in range(i+1, len(img_classes_list)):\n",
        "      class2_name = img_classes_list[j]\n",
        "      class2_images = query_dict[class2_name]\n",
        "      for image1_name in class1_images:\n",
        "        emb1 = img_to_embedding[image1_name]\n",
        "        for image2_name in class2_images:\n",
        "          emb2 = img_to_embedding[image2_name]\n",
        "          similarity = compute_cosine_similarity(emb1, emb2)\n",
        "          negative_similarities.append(similarity)\n",
        "\n",
        "  return negative_similarities\n"
      ],
      "metadata": {
        "id": "vNKd-2P6oETv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Пункт 3**: посчитаем косинусные расстояния между всеми парами лиц из query и distractors. Всего получится |query|*|distractors| пар."
      ],
      "metadata": {
        "id": "xEqgc2Kskmwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_query_distractors(query_embeddings, distractors_embeddings):\n",
        "  '''\n",
        "  compute cosine similarities between negative pairs from query and distractors\n",
        "  (stage 3)\n",
        "  params:\n",
        "    query_embeddings: list of embeddings corresponding to query_img_names\n",
        "    distractors_embeddings: list of embeddings corresponding to distractors_img_names\n",
        "  output:\n",
        "    list of floats: similarities between pairs of people (q, d), where q is\n",
        "                    embedding corresponding to photo from query, d —\n",
        "                    embedding corresponding to photo from distractors\n",
        "  '''\n",
        "  cosine_similarities = []\n",
        "\n",
        "  for query_embedding in tqdm.tqdm(query_embeddings):\n",
        "    for distractors_embedding in distractors_embeddings:\n",
        "      similarity = compute_cosine_similarity(query_embedding, distractors_embedding)\n",
        "      cosine_similarities.append(similarity)\n",
        "  return cosine_similarities"
      ],
      "metadata": {
        "id": "noqZx7hEoGR8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверка того, что код работает верно"
      ],
      "metadata": {
        "id": "lTJivkaoz7tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_query_dict = {\n",
        "    2876: ['1.jpg', '2.jpg', '3.jpg'],\n",
        "    5674: ['5.jpg'],\n",
        "    864:  ['9.jpg', '10.jpg'],\n",
        "}\n",
        "test_query_img_names = ['1.jpg', '2.jpg', '3.jpg', '5.jpg', '9.jpg', '10.jpg']\n",
        "test_query_embeddings = [\n",
        "                    [1.56, 6.45,  -7.68],\n",
        "                    [-1.1 , 6.11,  -3.0],\n",
        "                    [-0.06,-0.98,-1.29],\n",
        "                    [8.56, 1.45,  1.11],\n",
        "                    [0.7,  1.1,   -7.56],\n",
        "                    [0.05, 0.9,   -2.56],\n",
        "]\n",
        "\n",
        "test_distractors_img_names = ['11.jpg', '12.jpg', '13.jpg', '14.jpg', '15.jpg']\n",
        "\n",
        "test_distractors_embeddings = [\n",
        "                    [0.12, -3.23, -5.55],\n",
        "                    [-1,   -0.01, 1.22],\n",
        "                    [0.06, -0.23, 1.34],\n",
        "                    [-6.6, 1.45,  -1.45],\n",
        "                    [0.89,  1.98, 1.45],\n",
        "]\n",
        "\n",
        "test_cosine_query_pos = compute_cosine_query_pos(test_query_dict, test_query_img_names,\n",
        "                                            test_query_embeddings)\n",
        "test_cosine_query_neg = compute_cosine_query_neg(test_query_dict, test_query_img_names,\n",
        "                                            test_query_embeddings)\n",
        "test_cosine_query_distractors = compute_cosine_query_distractors(test_query_embeddings,\n",
        "                                                            test_distractors_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78lsfzA4z9Zw",
        "outputId": "fa29fd0b-b48c-433b-c0d1-817476046721"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:00<00:00, 475.78it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00, 6168.09it/s]\n",
            "100%|██████████| 6/6 [00:00<00:00, 8879.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iQIyWXmE_5tS"
      },
      "outputs": [],
      "source": [
        "true_cosine_query_pos = [0.8678237233650096, 0.21226104378511604,\n",
        "                         -0.18355866977496182, 0.9787437979250561]\n",
        "assert np.allclose(sorted(test_cosine_query_pos), sorted(true_cosine_query_pos)), \\\n",
        "      \"A mistake in compute_cosine_query_pos function\"\n",
        "\n",
        "true_cosine_query_neg = [0.15963231223161822, 0.8507997093616965, 0.9272761484302097,\n",
        "                         -0.0643994061127092, 0.5412660901220571, 0.701307100338029,\n",
        "                         -0.2372575528216902, 0.6941032794522218, 0.549425446066643,\n",
        "                         -0.011982733001947084, -0.0466679194884999]\n",
        "assert np.allclose(sorted(test_cosine_query_neg), sorted(true_cosine_query_neg)), \\\n",
        "      \"A mistake in compute_cosine_query_neg function\"\n",
        "\n",
        "true_cosine_query_distractors = [0.3371426578637511, -0.6866465610863652, -0.8456563512871669,\n",
        "                                 0.14530087113136106, 0.11410510307646118, -0.07265097629002357,\n",
        "                                 -0.24097699660707042,-0.5851992679925766, 0.4295494455718534,\n",
        "                                 0.37604478596058194, 0.9909483738948858, -0.5881093317868022,\n",
        "                                 -0.6829712976642919, 0.07546364489032083, -0.9130970963915521,\n",
        "                                 -0.17463101988684684, -0.5229363015558941, 0.1399896725311533,\n",
        "                                 -0.9258034013399499, 0.5295114163723346, 0.7811585442749943,\n",
        "                                 -0.8208760031249596, -0.9905139680301821, 0.14969764653247228,\n",
        "                                 -0.40749654525418444, 0.648660814944824, -0.7432584300096284,\n",
        "                                 -0.9839696492435877, 0.2498741082804709, -0.2661183373780491]\n",
        "assert np.allclose(sorted(test_cosine_query_distractors), sorted(true_cosine_query_distractors)), \\\n",
        "      \"A mistake in compute_cosine_query_distractors function\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors,\n",
        "               fpr=0.1):\n",
        "  '''\n",
        "  compute identification rate using precomputer cosine similarities between pairs\n",
        "  at given fpr\n",
        "  params:\n",
        "    cosine_query_pos: cosine similarities between positive pairs from query\n",
        "    cosine_query_neg: cosine similarities between negative pairs from query\n",
        "    cosine_query_distractors: cosine similarities between negative pairs\n",
        "                              from query and distractors\n",
        "    fpr: false positive rate at which to compute TPR\n",
        "  output:\n",
        "    float: threshold for given fpr\n",
        "    float: TPR at given FPR\n",
        "  '''\n",
        "  false_pairs_number = len(cosine_query_neg) + len(cosine_query_distractors) # количество false negative pairs\n",
        "  N = fpr * false_pairs_number # разрешённое количество false positives\n",
        "  N = int(round(N))\n",
        "  false_pairs = cosine_query_neg + cosine_query_distractors\n",
        "  false_pairs.sort(reverse = True) # отсортируем все значения косинусных расстояний false пар\n",
        "\n",
        "  # N — ое по счету значение расстояния зафиксируем как пороговое расстояние.\n",
        "  if N >= len(false_pairs):\n",
        "    threshold = false_pairs[-1]\n",
        "  else:\n",
        "    threshold = false_pairs[N]\n",
        "  # теперь посчитаем метрику tpr согласно пункту 7 описания метрики\n",
        "  TPR = len([True for i in cosine_query_pos if i >= threshold]) / len(cosine_query_pos)\n",
        "  return threshold, TPR"
      ],
      "metadata": {
        "id": "VE6HGAci2xGO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "С помощью ячеек ниже проверим, что метрика посчитана правильно"
      ],
      "metadata": {
        "id": "DHgge2HUl2Vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_thr = []\n",
        "test_tpr = []\n",
        "for fpr in [0.5, 0.3, 0.1]:\n",
        "  x, y = compute_ir(test_cosine_query_pos, test_cosine_query_neg,\n",
        "                    test_cosine_query_distractors, fpr=fpr)\n",
        "  test_thr.append(x)\n",
        "  test_tpr.append(y)"
      ],
      "metadata": {
        "id": "OyiA9nvI5-ni"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_thr = [-0.011982733001947084, 0.3371426578637511, 0.701307100338029]\n",
        "assert np.allclose(np.array(test_thr), np.array(true_thr)), \"A mistake in computing threshold\"\n",
        "\n",
        "true_tpr = [0.75, 0.5, 0.5]\n",
        "assert np.allclose(np.array(test_tpr), np.array(true_tpr)), \"A mistake in computing tpr\""
      ],
      "metadata": {
        "id": "zzOVG_zn3bY1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_ir(test_cosine_query_pos, test_cosine_query_neg, test_cosine_query_distractors,\n",
        "               fpr=0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoDLyG1e_ISL",
        "outputId": "8b32dd32-ee77-4c2a-fc81-d1d4d8e8afbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.33714265786375097, 0.5)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все проверки пройдены"
      ],
      "metadata": {
        "id": "dXHheXHbtbF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт обученной на CE модели resnet34, сохранённой на 50 - й эпохе обучения"
      ],
      "metadata": {
        "id": "P4Hff4PBhoSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaEflVA5qOYW",
        "outputId": "fe7e9d4b-0732-41ff-f9d5-aa77005cb4f4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet34()"
      ],
      "metadata": {
        "id": "DNAl7hPOHqat"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы уже обучили модель на CE Loss. Теперь загрузим состояние этой модели на последней, 50 - й эпохе. Загрузка производится на cpu."
      ],
      "metadata": {
        "id": "jJCvQKm8hsOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_model_state = torch.load('/content/model_state_dict_epoch_50.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjY6JgE8E6KG",
        "outputId": "77754f9e-e038-4a44-ba4f-7b1e34f1b7a2"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-30-dbafb3f7ae6c>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model_state = torch.load('/content/model_state_dict_epoch_50.pt', map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменяем fully connected слой на Dropout + линейный слой с нужным нам количеством классов - 500."
      ],
      "metadata": {
        "id": "HqJC4Cx2iRUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(p=0.5),\n",
        "    nn.Linear(in_features, 500)\n",
        ")\n",
        "model.load_state_dict(load_model_state['state_model']) # загружаем сохранённое состояние модели"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "id_7eQneRReU",
        "outputId": "106ebbb8-5889-4b29-c59a-e4458a8158e8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "backbone = torch.nn.Sequential(*list(model.children())[:-1]) # создаём итератор по всем слоям модели с помощью model.children() и оставляем все слои кроме последнего классификационного"
      ],
      "metadata": {
        "id": "ijEoNRLNH58P"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_folder_query = '/content/celebA_ir/celebA_ir/celebA_query'\n",
        "image_folder_distractors = '/content/celebA_ir/celebA_ir/celebA_distractors'"
      ],
      "metadata": {
        "id": "MM00V7P1pS0a"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Найдём эмбеддинги для наших данных"
      ],
      "metadata": {
        "id": "uMHrOEGPpiv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = compute_embeddings(backbone, query_img_names, image_folder_query)\n",
        "distractors_embeddings = compute_embeddings(backbone, distractors_img_names, image_folder_distractors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXImISAlpKO7",
        "outputId": "c550cf27-6afd-4e2d-a5da-50d082b88a52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1222/1222 [04:12<00:00,  4.84it/s]\n",
            "100%|██████████| 2001/2001 [07:14<00:00,  4.61it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем cosine_query_pos, cosine_query_neg, cosine_query_distractors для наших данных"
      ],
      "metadata": {
        "id": "P6fMS7DUphxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_pos = compute_cosine_query_pos(query_dict, query_img_names,\n",
        "                                            query_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpTkTZcPoCaZ",
        "outputId": "a63744f9-0526-48af-ffe7-f65a85787ae3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:00<00:00, 424.71it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_neg = compute_cosine_query_neg(query_dict, query_img_names,\n",
        "                                            query_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL5gJefFoEmA",
        "outputId": "f26d6f06-de37-4bef-e0d7-91777cfa57e6"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:06<00:00,  8.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_distractors = compute_cosine_query_distractors(query_embeddings,\n",
        "                                                            distractors_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euLN9YsWoGds",
        "outputId": "014c766e-5051-4c9a-94a0-05f72d4eb9b0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1222/1222 [00:24<00:00, 50.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посчитаем TPR@FPR для датасета с лицами для значений fpr = [0.5, 0.2, 0.1, 0.05]."
      ],
      "metadata": {
        "id": "XmY2YRHwqAWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for FPR in [0.5, 0.2, 0.1, 0.05]:\n",
        "  thr, tpr = compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors, fpr=FPR)\n",
        "  print(f'IR metric = {tpr} при frp = {FPR}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyC1kxYEpOKr",
        "outputId": "fd3c06c9-b9a7-430c-8f78-ef31642aa3e5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IR metric = 0.924325793084709 при frp = 0.5\n",
            "IR metric = 0.7471639154948713 при frp = 0.2\n",
            "IR metric = 0.6060050268324163 при frp = 0.1\n",
            "IR metric = 0.48373072481489027 при frp = 0.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт обученной на ArcLoss модели resnet34, сохранённой на 40 - й эпохе обучения"
      ],
      "metadata": {
        "id": "UwO2kdimqcHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "vWhjkRaRrfEs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArcFace(nn.Module):\n",
        "    \"\"\"\n",
        "    Имплементация ArcFace Loss (Additive Angular Margin Loss)\n",
        "\n",
        "    Параметры:\n",
        "        in_features (int): размерность входных эмбеддингов (512 в случае resnet34)\n",
        "        out_features (int): количество классов\n",
        "        s (float): scale коэффициент для логитов (default 64.0)\n",
        "        m (float): margin, добавляемый для улучшения разделимости классов (default 0.5)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features, s=64.0, m=0.5):\n",
        "        super(ArcFace, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.s = s\n",
        "        self.m = m\n",
        "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
        "        nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, input, label):\n",
        "        '''\n",
        "        Параметры:\n",
        "            input (torch.Tensor): эмбеддинги размерности [batch_size, in_features].\n",
        "            label (torch.Tensor): истинные метки классов размерности [batch_size].\n",
        "\n",
        "        Возвращает:\n",
        "            torch.Tensor: модифицированные логиты, которые мы уже будем подавать в softmax\n",
        "        '''\n",
        "\n",
        "        input = F.normalize(input, p = 2, dim = 1)\n",
        "        weight = F.normalize(self.weight, p = 2, dim = 1)\n",
        "        cosine = F.linear(input, weight)\n",
        "        sine = torch.sqrt(1.0 - torch.pow(cosine, 2))\n",
        "        alpha = cosine * math.cos(self.m) - sine * math.sin(self.m)\n",
        "        one_hot = torch.zeros(cosine.size(), device=device)\n",
        "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
        "        output = (one_hot * alpha) + ((1.0 - one_hot) * cosine)\n",
        "        output *= self.s\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "I3Dq0DkpmJzp"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArcFaceModel(nn.Module):\n",
        "    '''\n",
        "    Модель ArcFace, созданная на основе resnet34\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ArcFaceModel, self).__init__()\n",
        "\n",
        "        self.backbone = models.resnet34(weights = 'DEFAULT')\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, 512)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.arcface = ArcFace(512, 500)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        '''\n",
        "        Параметры:\n",
        "            x (torch.Tensor): входные изображения размерности [batch_size, 3, H, W]\n",
        "            labels (torch.Tensor, optional): истинные метки классов. При их передаче используется слой ArcFace\n",
        "        Возвращает:\n",
        "            torch.Tensor: возвращает эмбеддинги (при отсутствии labels), логиты (при наличии labels)\n",
        "        '''\n",
        "\n",
        "        x = self.backbone(x)\n",
        "        x = self.batch_norm1(x)\n",
        "        if labels is not None:\n",
        "            x = self.arcface(x, labels)\n",
        "        return x"
      ],
      "metadata": {
        "id": "mCjBmkDsmNqj"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model_state = torch.load('model_state_dict_epoch_40.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TH7NHZQrztj",
        "outputId": "5f4e35df-f088-4d0e-aaf1-e18df620d22c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-8a387a41f2c8>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  load_model_state = torch.load('model_state_dict_epoch_40.pt', map_location=torch.device('cpu'))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arcface_model = ArcFaceModel()"
      ],
      "metadata": {
        "id": "yqAbfZiCr0sx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "arcface_model.load_state_dict(load_model_state['state_model'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3AO1amCr2CD",
        "outputId": "ca9f8a0d-b89b-455c-ca31-267f9b592370"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_embeddings = compute_embeddings(arcface_model, query_img_names, image_folder_query)\n",
        "distractors_embeddings = compute_embeddings(arcface_model, distractors_img_names, image_folder_distractors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD_stO3Csd4w",
        "outputId": "c19dfae4-68ab-4082-ef3a-102b1bf5c694"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1222/1222 [04:08<00:00,  4.92it/s]\n",
            "100%|██████████| 2001/2001 [06:46<00:00,  4.92it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_pos = compute_cosine_query_pos(query_dict, query_img_names,\n",
        "                                            query_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h8f7GUusjYN",
        "outputId": "3dc37b84-9394-4db4-d50e-d5d806545fca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:00<00:00, 228.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_neg = compute_cosine_query_neg(query_dict, query_img_names,\n",
        "                                            query_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJAmLYv0slEn",
        "outputId": "b8fc8af2-20df-4748-84a5-48c8f2420f32"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51/51 [00:07<00:00,  6.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_query_distractors = compute_cosine_query_distractors(query_embeddings,\n",
        "                                                            distractors_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDlrjbzssmn8",
        "outputId": "dda8a685-e576-4072-d07e-afa0079f8df0"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1222/1222 [00:21<00:00, 57.35it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for FPR in [0.5, 0.2, 0.1, 0.05]:\n",
        "  thr, tpr = compute_ir(cosine_query_pos, cosine_query_neg, cosine_query_distractors, fpr=FPR)\n",
        "  print(f'IR metric = {tpr} при frp = {FPR}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfZQnbFxsoMI",
        "outputId": "98a61b0a-894b-4bcd-e0f0-38747fdcb329"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IR metric = 0.8578221588207323 при frp = 0.5\n",
            "IR metric = 0.6296447252224713 при frp = 0.2\n",
            "IR metric = 0.4740167108212757 при frp = 0.1\n",
            "IR metric = 0.3558861490387881 при frp = 0.05\n"
          ]
        }
      ]
    }
  ]
}